{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/debashisdotchatterjee/dax_ti_bayes_property_YM/blob/main/dax_ti_bayes_property_YM_Data_Code_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Physics-Informed Bayesian Surrogate Maps of Titanium-Alloy Mechanical Property Landscapes\n",
        "Using Manual MCMC Implementation on the DAX-Ti Database\n",
        "\n",
        "FIXED VERSION: Added numerical stability checks and regularization\n",
        "\n",
        "Author: Implementation based on methodology from Chatterjee (2025)\n",
        "Date: November 2025\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "from scipy.linalg import inv, cholesky, solve_triangular\n",
        "from scipy.stats import wishart, invwishart\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import os\n",
        "import json\n",
        "from datetime import datetime\n",
        "import zipfile\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Set plotting style\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.dpi'] = 150\n",
        "plt.rcParams['savefig.dpi'] = 300\n",
        "plt.rcParams['font.size'] = 10\n",
        "\n",
        "# Regularization constants for numerical stability\n",
        "EPSILON = 1e-6\n",
        "REG_LAMBDA = 1e-4\n",
        "\n",
        "# Create output directory\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "output_dir = f\"ti_alloy_bayesian_outputs_{timestamp}\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "os.makedirs(f\"{output_dir}/plots\", exist_ok=True)\n",
        "os.makedirs(f\"{output_dir}/tables\", exist_ok=True)\n",
        "os.makedirs(f\"{output_dir}/diagnostics\", exist_ok=True)\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"PHYSICS-INFORMED BAYESIAN MCMC FOR TI-ALLOY PROPERTY PREDICTION\")\n",
        "print(\"=\"*80)\n",
        "print(f\"\\nOutput directory: {output_dir}\\n\")\n",
        "\n",
        "# ============================================================================\n",
        "# HELPER FUNCTIONS WITH NUMERICAL STABILITY\n",
        "# ============================================================================\n",
        "\n",
        "def safe_inv(A, reg=REG_LAMBDA):\n",
        "    \"\"\"Safely invert matrix with regularization\"\"\"\n",
        "    try:\n",
        "        # Add small regularization to diagonal\n",
        "        A_reg = A + reg * np.eye(A.shape[0])\n",
        "        return inv(A_reg)\n",
        "    except:\n",
        "        # Fallback to higher regularization\n",
        "        A_reg = A + 10 * reg * np.eye(A.shape[0])\n",
        "        return inv(A_reg)\n",
        "\n",
        "def safe_cholesky(A, reg=REG_LAMBDA):\n",
        "    \"\"\"Safely compute Cholesky with regularization\"\"\"\n",
        "    try:\n",
        "        return cholesky(A, lower=True)\n",
        "    except:\n",
        "        # Add regularization until positive definite\n",
        "        A_reg = A.copy()\n",
        "        for i in range(10):\n",
        "            try:\n",
        "                A_reg = A + (10**i) * reg * np.eye(A.shape[0])\n",
        "                return cholesky(A_reg, lower=True)\n",
        "            except:\n",
        "                continue\n",
        "        # Last resort: use diagonal\n",
        "        return np.diag(np.sqrt(np.abs(np.diag(A)) + reg))\n",
        "\n",
        "def mvn_sample(mean, cov):\n",
        "    \"\"\"Sample from multivariate normal with numerical stability\"\"\"\n",
        "    L = safe_cholesky(cov)\n",
        "    z = np.random.randn(len(mean))\n",
        "    return mean + L @ z\n",
        "\n",
        "def matrix_normal_sample(M, U, V):\n",
        "    \"\"\"Sample from Matrix Normal with numerical stability\"\"\"\n",
        "    G = np.random.randn(M.shape[0], M.shape[1])\n",
        "    L_U = safe_cholesky(U)\n",
        "    L_V = safe_cholesky(V)\n",
        "    return M + L_U @ G @ L_V.T\n",
        "\n",
        "def inv_gamma_sample(a, b):\n",
        "    \"\"\"Sample from Inverse-Gamma(a, b)\"\"\"\n",
        "    return 1.0 / np.random.gamma(a, 1.0/b)\n",
        "\n",
        "def inv_wishart_sample(df, scale):\n",
        "    \"\"\"Sample from Inverse-Wishart with stability checks\"\"\"\n",
        "    try:\n",
        "        return invwishart.rvs(df=df, scale=scale)\n",
        "    except:\n",
        "        # Fallback: regularize scale matrix\n",
        "        K = scale.shape[0]\n",
        "        scale_reg = scale + REG_LAMBDA * np.eye(K)\n",
        "        return invwishart.rvs(df=df, scale=scale_reg)\n",
        "\n",
        "def check_matrix_health(M, name=\"Matrix\"):\n",
        "    \"\"\"Check if matrix has NaN or Inf values\"\"\"\n",
        "    if np.any(np.isnan(M)) or np.any(np.isinf(M)):\n",
        "        print(f\"  WARNING: {name} contains NaN/Inf values!\")\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "# ============================================================================\n",
        "# 1. DATA LOADING AND PREPROCESSING\n",
        "# ============================================================================\n",
        "\n",
        "print(\"Step 1: Loading and preprocessing DAX-Ti database...\")\n",
        "\n",
        "# Load data\n",
        "df = pd.read_csv('dax-ti-static.csv')\n",
        "print(f\"  - Loaded {len(df)} alloy entries\")\n",
        "\n",
        "# Select mechanical properties\n",
        "property_names = ['YM', 'YS', 'UTS', 'DAR', 'HV']\n",
        "property_errors = ['YM_err', 'YS_err', 'UTS_err', 'DAR_err', 'HV_err']\n",
        "K = len(property_names)\n",
        "\n",
        "# Extract composition features\n",
        "def parse_composition(formula_str):\n",
        "    \"\"\"Extract element fractions from composition string\"\"\"\n",
        "    elements = {}\n",
        "    try:\n",
        "        parts = str(formula_str).replace(' ', '').split()\n",
        "        for part in parts:\n",
        "            elem = ''.join([c for c in part if c.isalpha()])\n",
        "            frac_str = ''.join([c for c in part if c.isdigit() or c == '.'])\n",
        "            if elem and frac_str:\n",
        "                frac = float(frac_str)\n",
        "                if frac > 0:\n",
        "                    elements[elem] = frac\n",
        "    except:\n",
        "        pass\n",
        "    return elements\n",
        "\n",
        "# Parse compositions\n",
        "df['parsed_comp'] = df['composition'].apply(parse_composition)\n",
        "\n",
        "# Extract key elements\n",
        "key_elements = ['Ti', 'Nb', 'Zr', 'Fe', 'Mo', 'Ta', 'Sn', 'V']\n",
        "for elem in key_elements:\n",
        "    df[f'frac_{elem}'] = df['parsed_comp'].apply(lambda x: x.get(elem, 0.0))\n",
        "\n",
        "# Clean data\n",
        "df_clean = df.copy()\n",
        "df_clean.loc[df_clean['DAR'] < 0, 'DAR'] = np.nan\n",
        "df_clean.loc[df_clean['DAR'] < 0, 'DAR_err'] = np.nan\n",
        "\n",
        "# Log-transform properties (handle zeros)\n",
        "for prop in property_names:\n",
        "    vals = df_clean[prop].values\n",
        "    vals[vals <= 0] = np.nan  # Set non-positive to NaN\n",
        "    df_clean[f'log_{prop}'] = np.log(vals)\n",
        "\n",
        "# Prepare covariates\n",
        "covariate_cols = ['moe'] + [f'frac_{e}' for e in key_elements if e != 'Ti']\n",
        "X_raw = df_clean[covariate_cols].copy()\n",
        "\n",
        "# Add intercept\n",
        "X_raw.insert(0, 'intercept', 1.0)\n",
        "\n",
        "# Standardize (except intercept)\n",
        "X_std = X_raw.copy()\n",
        "for col in X_raw.columns[1:]:\n",
        "    if X_raw[col].std() > EPSILON:\n",
        "        X_std[col] = (X_raw[col] - X_raw[col].mean()) / (X_raw[col].std() + EPSILON)\n",
        "    else:\n",
        "        X_std[col] = 0.0\n",
        "\n",
        "X = X_std.values  # n x p\n",
        "n, p = X.shape\n",
        "\n",
        "print(f\"  - Prepared {n} samples with {p} covariates\")\n",
        "print(f\"  - Covariates: {list(X_std.columns)}\")\n",
        "\n",
        "# Extract observed responses (log-transformed)\n",
        "Z_obs = df_clean[[f'log_{prop}' for prop in property_names]].values  # n x K\n",
        "S_obs = df_clean[[f'{prop}_err' for prop in property_names]].values  # n x K\n",
        "\n",
        "# Approximate error on log-scale: sigma_log(y) ≈ sigma_y / y\n",
        "for k, prop in enumerate(property_names):\n",
        "    valid_mask = ~np.isnan(df_clean[prop]) & (df_clean[prop] > 0)\n",
        "    S_obs[valid_mask, k] = S_obs[valid_mask, k] / (df_clean.loc[valid_mask, prop] + EPSILON)\n",
        "\n",
        "# Missing data indicators\n",
        "R = (~np.isnan(Z_obs)).astype(int)  # n x K\n",
        "\n",
        "# Stability class encoding\n",
        "stability_classes = ['stable', 'meta', 'near', 'rich', 'other']\n",
        "df_clean['stability_idx'] = df_clean['moe_class'].map(\n",
        "    {cls: i for i, cls in enumerate(stability_classes)}\n",
        ").fillna(4)  # 'other' for missing\n",
        "\n",
        "z_class = df_clean['stability_idx'].values.astype(int)\n",
        "J = len(stability_classes)\n",
        "\n",
        "print(f\"  - Stability classes: {stability_classes}\")\n",
        "class_dist = {cls: int((z_class == i).sum()) for i, cls in enumerate(stability_classes)}\n",
        "print(f\"  - Class distribution: {class_dist}\")\n",
        "\n",
        "# Missing data summary\n",
        "missing_fraction = 1 - R.mean(axis=0)\n",
        "print(f\"\\n  Missing data per property:\")\n",
        "for i, prop in enumerate(property_names):\n",
        "    print(f\"    {prop}: {missing_fraction[i]*100:.1f}%\")\n",
        "\n",
        "# ============================================================================\n",
        "# 2. INITIALIZE PARAMETERS WITH SAFE DEFAULTS\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\nStep 2: Initializing MCMC parameters...\")\n",
        "\n",
        "# Impute missing values with column means\n",
        "Z_init = Z_obs.copy()\n",
        "for k in range(K):\n",
        "    col_mean = np.nanmean(Z_obs[:, k])\n",
        "    if np.isnan(col_mean):\n",
        "        col_mean = 0.0\n",
        "    Z_init[np.isnan(Z_init[:, k]), k] = col_mean\n",
        "\n",
        "# Initialize S with small values for missing entries\n",
        "S_init = S_obs.copy()\n",
        "S_init[np.isnan(S_init)] = 0.1\n",
        "\n",
        "# Empirical pooled regression (OLS with regularization)\n",
        "XtX = X.T @ X\n",
        "XtX_inv = safe_inv(XtX, reg=REG_LAMBDA)\n",
        "B_pool = XtX_inv @ X.T @ Z_init\n",
        "residuals = Z_init - X @ B_pool\n",
        "\n",
        "# Empirical covariance (regularized)\n",
        "Sigma_emp = (residuals.T @ residuals) / (n - p - 1)\n",
        "Sigma_emp = Sigma_emp + 0.01 * np.eye(K)  # Strong regularization\n",
        "\n",
        "# Initialize class-specific coefficients\n",
        "B_classes = {j: B_pool + 0.01 * np.random.randn(p, K) for j in range(J)}\n",
        "\n",
        "# Initialize global mean\n",
        "M = B_pool.copy()\n",
        "\n",
        "# Initialize Lambda\n",
        "Lambda = np.eye(K) * 0.5\n",
        "\n",
        "# Initialize tau^2\n",
        "tau2 = 0.5\n",
        "\n",
        "# Initialize Sigma\n",
        "Sigma = Sigma_emp.copy()\n",
        "\n",
        "# Initialize latent Z\n",
        "Z = Z_init.copy()\n",
        "\n",
        "print(\"  - Initialized all parameters with regularization\")\n",
        "\n",
        "# ============================================================================\n",
        "# 3. HYPERPARAMETERS\n",
        "# ============================================================================\n",
        "\n",
        "# Prior for M\n",
        "c0 = 1.0\n",
        "\n",
        "# Prior for Sigma (more conservative)\n",
        "nu0 = K + 5  # More informative\n",
        "S0 = Sigma_emp * (nu0 - K - 1)\n",
        "S0 = S0 + 0.1 * np.eye(K)  # Extra regularization\n",
        "\n",
        "# Prior for Lambda\n",
        "nu_Lambda = K + 5  # More informative\n",
        "S_Lambda = np.eye(K) * 1.0\n",
        "\n",
        "# Prior for tau^2\n",
        "a_tau = 3.0  # More informative\n",
        "b_tau = 1.0\n",
        "\n",
        "print(f\"\\n  Hyperparameters:\")\n",
        "print(f\"    c0 (M prior scale): {c0}\")\n",
        "print(f\"    nu0 (Sigma prior df): {nu0}\")\n",
        "print(f\"    nu_Lambda (Lambda prior df): {nu_Lambda}\")\n",
        "print(f\"    a_tau, b_tau: {a_tau}, {b_tau}\")\n",
        "\n",
        "# ============================================================================\n",
        "# 4. MCMC SAMPLER (WITH STABILITY CHECKS)\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\nStep 3: Running MCMC sampler...\")\n",
        "print(\"  This may take several minutes...\\n\")\n",
        "\n",
        "# MCMC settings\n",
        "n_iter = 5000\n",
        "n_burn = 1000\n",
        "n_thin = 2\n",
        "n_save = (n_iter - n_burn) // n_thin\n",
        "\n",
        "# Storage\n",
        "samples = {\n",
        "    'B': {j: np.zeros((n_save, p, K)) for j in range(J)},\n",
        "    'M': np.zeros((n_save, p, K)),\n",
        "    'Sigma': np.zeros((n_save, K, K)),\n",
        "    'Lambda': np.zeros((n_save, K, K)),\n",
        "    'tau2': np.zeros(n_save),\n",
        "    'Z': np.zeros((n_save, n, K)),\n",
        "}\n",
        "\n",
        "# Progress bar\n",
        "def progress_bar(iteration, total, bar_length=50):\n",
        "    percent = float(iteration) / total\n",
        "    arrow = '=' * int(round(percent * bar_length))\n",
        "    spaces = ' ' * (bar_length - len(arrow))\n",
        "    print(f'\\r  Progress: [{arrow}{spaces}] {int(percent * 100)}% ({iteration}/{total})', end='', flush=True)\n",
        "\n",
        "# MCMC loop\n",
        "save_idx = 0\n",
        "n_errors = 0\n",
        "max_errors = 100\n",
        "\n",
        "for iter_num in range(n_iter):\n",
        "    progress_bar(iter_num + 1, n_iter)\n",
        "\n",
        "    try:\n",
        "        # --------------------------------------------------------------------\n",
        "        # Step 1: Sample latent Z for missing/observed entries\n",
        "        # --------------------------------------------------------------------\n",
        "        for i in range(n):\n",
        "            class_i = z_class[i]\n",
        "            B_i = B_classes[class_i]\n",
        "            mean_i = X[i, :] @ B_i\n",
        "\n",
        "            for k in range(K):\n",
        "                if R[i, k] == 0:  # Missing\n",
        "                    # Sample from prior\n",
        "                    var_k = max(Sigma[k, k], EPSILON)\n",
        "                    Z[i, k] = np.random.normal(mean_i[k], np.sqrt(var_k))\n",
        "                else:  # Observed\n",
        "                    # Sample from posterior given observation\n",
        "                    obs_var = max(S_init[i, k]**2, EPSILON)\n",
        "                    sigma_k = max(Sigma[k, k], EPSILON)\n",
        "                    post_var = 1.0 / (1.0/sigma_k + 1.0/obs_var)\n",
        "                    post_mean = post_var * (mean_i[k]/sigma_k + Z_obs[i, k]/obs_var)\n",
        "                    Z[i, k] = np.random.normal(post_mean, np.sqrt(post_var))\n",
        "\n",
        "        # --------------------------------------------------------------------\n",
        "        # Step 2: Sample class-specific B_j\n",
        "        # --------------------------------------------------------------------\n",
        "        Sigma_inv = safe_inv(Sigma)\n",
        "        XtX_inv_tau = XtX_inv * tau2\n",
        "        Lambda_safe = Lambda + REG_LAMBDA * np.eye(K)\n",
        "\n",
        "        for j in range(J):\n",
        "            idx_j = np.where(z_class == j)[0]\n",
        "\n",
        "            if len(idx_j) == 0:\n",
        "                # No data, sample from prior\n",
        "                B_classes[j] = matrix_normal_sample(M, XtX_inv_tau, Lambda_safe)\n",
        "                continue\n",
        "\n",
        "            X_j = X[idx_j, :]\n",
        "            Z_j = Z[idx_j, :]\n",
        "            n_j = len(idx_j)\n",
        "\n",
        "            # Use simplified conjugate update (avoiding large Kronecker products)\n",
        "            # Posterior for each column separately (approximate)\n",
        "            B_j_new = np.zeros((p, K))\n",
        "            for k_idx in range(K):\n",
        "                # For property k_idx\n",
        "                z_jk = Z_j[:, k_idx]\n",
        "\n",
        "                # Prior precision (from M)\n",
        "                prior_prec = XtX_inv_tau / Lambda_safe[k_idx, k_idx]\n",
        "                prior_prec_mat = safe_inv(prior_prec * np.eye(p))\n",
        "\n",
        "                # Likelihood precision\n",
        "                like_prec = (X_j.T @ X_j) / Sigma[k_idx, k_idx]\n",
        "\n",
        "                # Posterior\n",
        "                post_prec = prior_prec_mat + like_prec\n",
        "                post_cov = safe_inv(post_prec)\n",
        "\n",
        "                prior_mean = M[:, k_idx]\n",
        "                like_contrib = X_j.T @ z_jk / Sigma[k_idx, k_idx]\n",
        "\n",
        "                post_mean = post_cov @ (prior_prec_mat @ prior_mean + like_contrib)\n",
        "\n",
        "                # Sample\n",
        "                B_j_new[:, k_idx] = mvn_sample(post_mean, post_cov)\n",
        "\n",
        "            B_classes[j] = B_j_new\n",
        "\n",
        "        # --------------------------------------------------------------------\n",
        "        # Step 3: Sample global mean M\n",
        "        # --------------------------------------------------------------------\n",
        "        # Simplified: average of class-specific B_j with shrinkage toward B_pool\n",
        "        B_mean = np.mean([B_classes[j] for j in range(J)], axis=0)\n",
        "        shrinkage = 0.9  # Shrink toward empirical estimate\n",
        "        M = shrinkage * B_mean + (1 - shrinkage) * B_pool\n",
        "\n",
        "        # Add small noise for mixing\n",
        "        M = M + 0.01 * np.random.randn(p, K)\n",
        "\n",
        "        # --------------------------------------------------------------------\n",
        "        # Step 4: Sample Sigma (residual covariance)\n",
        "        # --------------------------------------------------------------------\n",
        "        residuals_all = []\n",
        "        for i in range(n):\n",
        "            class_i = z_class[i]\n",
        "            B_i = B_classes[class_i]\n",
        "            resid_i = Z[i, :] - X[i, :] @ B_i\n",
        "            residuals_all.append(resid_i)\n",
        "\n",
        "        residuals_all = np.array(residuals_all)\n",
        "        S_resid = residuals_all.T @ residuals_all\n",
        "\n",
        "        df_post = nu0 + n\n",
        "        scale_post_inv = safe_inv(S0) + S_resid\n",
        "        scale_post = safe_inv(scale_post_inv)\n",
        "\n",
        "        Sigma = inv_wishart_sample(df_post, scale_post)\n",
        "\n",
        "        # Regularize to ensure positive definiteness\n",
        "        Sigma = Sigma + REG_LAMBDA * np.eye(K)\n",
        "\n",
        "        # --------------------------------------------------------------------\n",
        "        # Step 5: Sample Lambda (coefficient coupling)\n",
        "        # --------------------------------------------------------------------\n",
        "        B_deviations_sum = np.zeros((K, K))\n",
        "        for j in range(J):\n",
        "            B_dev = B_classes[j] - M\n",
        "            B_deviations_sum += B_dev.T @ XtX @ B_dev / tau2\n",
        "\n",
        "        df_Lambda_post = nu_Lambda + J * p\n",
        "        scale_Lambda_post_inv = safe_inv(S_Lambda) + B_deviations_sum\n",
        "        scale_Lambda_post = safe_inv(scale_Lambda_post_inv)\n",
        "\n",
        "        Lambda = inv_wishart_sample(df_Lambda_post, scale_Lambda_post)\n",
        "\n",
        "        # Regularize\n",
        "        Lambda = Lambda + REG_LAMBDA * np.eye(K)\n",
        "\n",
        "        # --------------------------------------------------------------------\n",
        "        # Step 6: Sample tau^2\n",
        "        # --------------------------------------------------------------------\n",
        "        a_post = a_tau + 0.5 * J * p * K\n",
        "\n",
        "        Lambda_inv = safe_inv(Lambda)\n",
        "        b_sum = 0\n",
        "        for j in range(J):\n",
        "            B_dev = B_classes[j] - M\n",
        "            b_sum += np.trace(B_dev.T @ XtX @ B_dev @ Lambda_inv)\n",
        "\n",
        "        b_post = b_tau + 0.5 * b_sum\n",
        "        b_post = max(b_post, EPSILON)  # Ensure positive\n",
        "\n",
        "        tau2 = inv_gamma_sample(a_post, b_post)\n",
        "        tau2 = np.clip(tau2, 0.01, 10.0)  # Bound for stability\n",
        "\n",
        "        # --------------------------------------------------------------------\n",
        "        # Store samples\n",
        "        # --------------------------------------------------------------------\n",
        "        if iter_num >= n_burn and (iter_num - n_burn) % n_thin == 0:\n",
        "            for j in range(J):\n",
        "                samples['B'][j][save_idx, :, :] = B_classes[j]\n",
        "            samples['M'][save_idx, :, :] = M\n",
        "            samples['Sigma'][save_idx, :, :] = Sigma\n",
        "            samples['Lambda'][save_idx, :, :] = Lambda\n",
        "            samples['tau2'][save_idx] = tau2\n",
        "            samples['Z'][save_idx, :, :] = Z\n",
        "            save_idx += 1\n",
        "\n",
        "    except Exception as e:\n",
        "        n_errors += 1\n",
        "        if n_errors > max_errors:\n",
        "            print(f\"\\n  ERROR: Too many numerical errors ({n_errors}). Stopping.\")\n",
        "            print(f\"  Last error: {str(e)}\")\n",
        "            break\n",
        "        # Skip this iteration and continue\n",
        "        continue\n",
        "\n",
        "print(\"\\n  MCMC sampling completed!\")\n",
        "print(f\"  Total numerical errors handled: {n_errors}\")\n",
        "\n",
        "# Trim samples if stopped early\n",
        "if save_idx < n_save:\n",
        "    print(f\"  Note: Collected {save_idx} samples (expected {n_save})\")\n",
        "    for key in samples:\n",
        "        if key == 'B':\n",
        "            for j in range(J):\n",
        "                samples['B'][j] = samples['B'][j][:save_idx]\n",
        "        else:\n",
        "            samples[key] = samples[key][:save_idx]\n",
        "    n_save = save_idx\n",
        "\n",
        "# ============================================================================\n",
        "# 5. POSTERIOR SUMMARIES\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\nStep 4: Computing posterior summaries...\")\n",
        "\n",
        "def posterior_summary(samples_array, axis=0):\n",
        "    \"\"\"Compute mean, std, and 95% CI\"\"\"\n",
        "    mean = np.mean(samples_array, axis=axis)\n",
        "    std = np.std(samples_array, axis=axis)\n",
        "    ci_lower = np.percentile(samples_array, 2.5, axis=axis)\n",
        "    ci_upper = np.percentile(samples_array, 97.5, axis=axis)\n",
        "    return mean, std, ci_lower, ci_upper\n",
        "\n",
        "# Global mean M\n",
        "M_mean, M_std, M_lower, M_upper = posterior_summary(samples['M'], axis=0)\n",
        "\n",
        "# Residual covariance Sigma\n",
        "Sigma_mean = np.mean(samples['Sigma'], axis=0)\n",
        "Sigma_std = np.std(samples['Sigma'], axis=0)\n",
        "\n",
        "# Coefficient coupling Lambda\n",
        "Lambda_mean = np.mean(samples['Lambda'], axis=0)\n",
        "\n",
        "# Global shrinkage tau^2\n",
        "tau2_mean = np.mean(samples['tau2'])\n",
        "tau2_std = np.std(samples['tau2'])\n",
        "\n",
        "print(f\"  - Posterior mean tau^2: {tau2_mean:.4f} ± {tau2_std:.4f}\")\n",
        "print(f\"  - Posterior mean Sigma diagonal: {np.diag(Sigma_mean)}\")\n",
        "\n",
        "# Save summaries\n",
        "summaries = {\n",
        "    'tau2_mean': float(tau2_mean),\n",
        "    'tau2_std': float(tau2_std),\n",
        "    'Sigma_mean': Sigma_mean.tolist(),\n",
        "    'Lambda_mean': Lambda_mean.tolist(),\n",
        "    'n_samples': int(n_save),\n",
        "    'n_errors': int(n_errors),\n",
        "}\n",
        "\n",
        "with open(f\"{output_dir}/posterior_summaries.json\", 'w') as f:\n",
        "    json.dump(summaries, f, indent=2)\n",
        "\n",
        "# ============================================================================\n",
        "# 6. POSTERIOR PREDICTIVE DISTRIBUTIONS\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\nStep 5: Computing posterior predictive distributions...\")\n",
        "\n",
        "# Select test cases\n",
        "test_indices = [0, 50, 100, 150, 200]\n",
        "test_indices = [i for i in test_indices if i < n]  # Ensure valid\n",
        "n_test = len(test_indices)\n",
        "\n",
        "predictive_samples = np.zeros((n_save, n_test, K))\n",
        "\n",
        "for s in range(n_save):\n",
        "    for t_idx, i in enumerate(test_indices):\n",
        "        class_i = z_class[i]\n",
        "        B_i = samples['B'][class_i][s, :, :]\n",
        "        mean_pred = X[i, :] @ B_i\n",
        "        Sigma_s = samples['Sigma'][s, :, :]\n",
        "        predictive_samples[s, t_idx, :] = mvn_sample(mean_pred, Sigma_s)\n",
        "\n",
        "# Back-transform\n",
        "predictive_samples_original = np.exp(predictive_samples)\n",
        "\n",
        "print(f\"  - Generated {n_save} predictive samples for {n_test} test cases\")\n",
        "\n",
        "# ============================================================================\n",
        "# 7. DIAGNOSTICS (SIMPLIFIED)\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\nStep 6: Computing MCMC diagnostics...\")\n",
        "\n",
        "# Trace plots\n",
        "fig, axes = plt.subplots(3, 2, figsize=(12, 10))\n",
        "fig.suptitle(\"MCMC Trace Plots\", fontsize=14, fontweight='bold')\n",
        "\n",
        "axes[0, 0].plot(samples['tau2'], alpha=0.7, linewidth=0.5)\n",
        "axes[0, 0].set_title(r'$\\tau^2$ (Global Shrinkage)')\n",
        "axes[0, 0].set_xlabel('Iteration')\n",
        "axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "for k in range(min(3, K)):\n",
        "    sigma_kk = samples['Sigma'][:, k, k]\n",
        "    axes[0, 1].plot(sigma_kk, alpha=0.7, linewidth=0.5, label=property_names[k])\n",
        "axes[0, 1].set_title(r'$\\Sigma$ Diagonal Elements')\n",
        "axes[0, 1].set_xlabel('Iteration')\n",
        "axes[0, 1].legend(fontsize=8)\n",
        "axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "for k in range(min(3, K)):\n",
        "    M_0k = samples['M'][:, 0, k]\n",
        "    axes[1, 0].plot(M_0k, alpha=0.7, linewidth=0.5, label=property_names[k])\n",
        "axes[1, 0].set_title('M Intercept Coefficients')\n",
        "axes[1, 0].set_xlabel('Iteration')\n",
        "axes[1, 0].legend(fontsize=8)\n",
        "axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "for k in range(min(3, K)):\n",
        "    B_0_0k = samples['B'][0][:, 0, k]\n",
        "    axes[1, 1].plot(B_0_0k, alpha=0.7, linewidth=0.5, label=property_names[k])\n",
        "axes[1, 1].set_title('B[stable] Intercept Coefficients')\n",
        "axes[1, 1].set_xlabel('Iteration')\n",
        "axes[1, 1].legend(fontsize=8)\n",
        "axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "for k in range(min(3, K)):\n",
        "    lambda_kk = samples['Lambda'][:, k, k]\n",
        "    axes[2, 0].plot(lambda_kk, alpha=0.7, linewidth=0.5, label=property_names[k])\n",
        "axes[2, 0].set_title(r'$\\Lambda$ Diagonal Elements')\n",
        "axes[2, 0].set_xlabel('Iteration')\n",
        "axes[2, 0].legend(fontsize=8)\n",
        "axes[2, 0].grid(True, alpha=0.3)\n",
        "\n",
        "for k in range(min(3, K)):\n",
        "    Z_0k = samples['Z'][:, 0, k]\n",
        "    axes[2, 1].plot(Z_0k, alpha=0.7, linewidth=0.5, label=property_names[k])\n",
        "axes[2, 1].set_title('Latent Z[0] (Log Properties)')\n",
        "axes[2, 1].set_xlabel('Iteration')\n",
        "axes[2, 1].legend(fontsize=8)\n",
        "axes[2, 1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(f\"{output_dir}/diagnostics/trace_plots.png\", bbox_inches='tight')\n",
        "print(f\"  - Saved: diagnostics/trace_plots.png\")\n",
        "plt.close()\n",
        "\n",
        "# Autocorrelation (simplified)\n",
        "def autocorr(x, max_lag=50):\n",
        "    \"\"\"Compute autocorrelation\"\"\"\n",
        "    x = x - np.mean(x)\n",
        "    c0 = np.dot(x, x) / len(x)\n",
        "    if c0 == 0:\n",
        "        return np.ones(max_lag)\n",
        "    acf = [1.0] + [np.dot(x[:-lag], x[lag:]) / len(x) / c0 for lag in range(1, max_lag)]\n",
        "    return np.array(acf)\n",
        "\n",
        "fig, axes = plt.subplots(2, 3, figsize=(14, 8))\n",
        "fig.suptitle(\"MCMC Autocorrelation Plots\", fontsize=14, fontweight='bold')\n",
        "\n",
        "acf_tau2 = autocorr(samples['tau2'])\n",
        "axes[0, 0].plot(acf_tau2, marker='o', markersize=3)\n",
        "axes[0, 0].axhline(0, color='k', linestyle='--', linewidth=0.5)\n",
        "axes[0, 0].set_title(r'ACF: $\\tau^2$')\n",
        "axes[0, 0].set_xlabel('Lag')\n",
        "axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "acf_sigma00 = autocorr(samples['Sigma'][:, 0, 0])\n",
        "axes[0, 1].plot(acf_sigma00, marker='o', markersize=3)\n",
        "axes[0, 1].axhline(0, color='k', linestyle='--', linewidth=0.5)\n",
        "axes[0, 1].set_title(r'ACF: $\\Sigma_{11}$')\n",
        "axes[0, 1].set_xlabel('Lag')\n",
        "axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "acf_M00 = autocorr(samples['M'][:, 0, 0])\n",
        "axes[0, 2].plot(acf_M00, marker='o', markersize=3)\n",
        "axes[0, 2].axhline(0, color='k', linestyle='--', linewidth=0.5)\n",
        "axes[0, 2].set_title(r'ACF: $M_{11}$')\n",
        "axes[0, 2].set_xlabel('Lag')\n",
        "axes[0, 2].grid(True, alpha=0.3)\n",
        "\n",
        "acf_B000 = autocorr(samples['B'][0][:, 0, 0])\n",
        "axes[1, 0].plot(acf_B000, marker='o', markersize=3)\n",
        "axes[1, 0].axhline(0, color='k', linestyle='--', linewidth=0.5)\n",
        "axes[1, 0].set_title(r'ACF: $B_{stable,11}$')\n",
        "axes[1, 0].set_xlabel('Lag')\n",
        "axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "acf_Lambda00 = autocorr(samples['Lambda'][:, 0, 0])\n",
        "axes[1, 1].plot(acf_Lambda00, marker='o', markersize=3)\n",
        "axes[1, 1].axhline(0, color='k', linestyle='--', linewidth=0.5)\n",
        "axes[1, 1].set_title(r'ACF: $\\Lambda_{11}$')\n",
        "axes[1, 1].set_xlabel('Lag')\n",
        "axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "acf_Z00 = autocorr(samples['Z'][:, 0, 0])\n",
        "axes[1, 2].plot(acf_Z00, marker='o', markersize=3)\n",
        "axes[1, 2].axhline(0, color='k', linestyle='--', linewidth=0.5)\n",
        "axes[1, 2].set_title(r'ACF: $Z_{11}$')\n",
        "axes[1, 2].set_xlabel('Lag')\n",
        "axes[1, 2].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(f\"{output_dir}/diagnostics/autocorrelation_plots.png\", bbox_inches='tight')\n",
        "print(f\"  - Saved: diagnostics/autocorrelation_plots.png\")\n",
        "plt.close()\n",
        "\n",
        "# Effective sample size\n",
        "def ess(x, max_lag=None):\n",
        "    \"\"\"Estimate effective sample size\"\"\"\n",
        "    if max_lag is None:\n",
        "        max_lag = min(len(x) // 2, 500)\n",
        "    acf_vals = autocorr(x, max_lag)\n",
        "    cutoff = np.where(acf_vals < 0.05)[0]\n",
        "    if len(cutoff) > 0:\n",
        "        tau = 1 + 2 * np.sum(acf_vals[1:cutoff[0]])\n",
        "    else:\n",
        "        tau = 1 + 2 * np.sum(acf_vals[1:])\n",
        "    return len(x) / max(tau, 1.0)\n",
        "\n",
        "ess_tau2 = ess(samples['tau2'])\n",
        "ess_sigma00 = ess(samples['Sigma'][:, 0, 0])\n",
        "ess_M00 = ess(samples['M'][:, 0, 0])\n",
        "\n",
        "print(f\"\\n  Effective Sample Sizes:\")\n",
        "print(f\"    tau^2: {ess_tau2:.0f}\")\n",
        "print(f\"    Sigma[0,0]: {ess_sigma00:.0f}\")\n",
        "print(f\"    M[0,0]: {ess_M00:.0f}\")\n",
        "\n",
        "# R-hat (split-chain approximation)\n",
        "def split_chain_rhat(chain):\n",
        "    \"\"\"Approximate R-hat\"\"\"\n",
        "    n = len(chain)\n",
        "    if n < 4:\n",
        "        return 1.0\n",
        "    chain1 = chain[:n//2]\n",
        "    chain2 = chain[n//2:]\n",
        "\n",
        "    mean1 = np.mean(chain1)\n",
        "    mean2 = np.mean(chain2)\n",
        "    var1 = np.var(chain1, ddof=1)\n",
        "    var2 = np.var(chain2, ddof=1)\n",
        "\n",
        "    W = (var1 + var2) / 2\n",
        "    B = ((mean1 - mean2)**2) * (n//2)\n",
        "\n",
        "    if W == 0:\n",
        "        return 1.0\n",
        "\n",
        "    var_plus = (n//2 - 1) / (n//2) * W + B / (n//2)\n",
        "    rhat = np.sqrt(var_plus / W)\n",
        "    return rhat\n",
        "\n",
        "rhat_tau2 = split_chain_rhat(samples['tau2'])\n",
        "rhat_sigma00 = split_chain_rhat(samples['Sigma'][:, 0, 0])\n",
        "rhat_M00 = split_chain_rhat(samples['M'][:, 0, 0])\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(8, 5))\n",
        "params = [r'$\\tau^2$', r'$\\Sigma_{11}$', r'$M_{11}$']\n",
        "rhat_values = [rhat_tau2, rhat_sigma00, rhat_M00]\n",
        "\n",
        "bars = ax.barh(params, rhat_values, color=['green' if r < 1.1 else 'orange' for r in rhat_values], alpha=0.8)\n",
        "ax.axvline(1.0, color='blue', linestyle='--', linewidth=2, label='Target (1.0)')\n",
        "ax.axvline(1.1, color='red', linestyle='--', linewidth=2, label='Threshold (1.1)')\n",
        "ax.set_xlabel(r'$\\hat{R}$ (Split-Chain)', fontsize=11)\n",
        "ax.set_title('Convergence Diagnostic', fontsize=12, fontweight='bold')\n",
        "ax.legend(fontsize=9)\n",
        "ax.grid(True, alpha=0.3, axis='x')\n",
        "\n",
        "for i, v in enumerate(rhat_values):\n",
        "    ax.text(v + 0.01, i, f'{v:.3f}', va='center', fontsize=9)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(f\"{output_dir}/diagnostics/rhat_diagnostics.png\", bbox_inches='tight')\n",
        "print(f\"  - Saved: diagnostics/rhat_diagnostics.png\")\n",
        "plt.close()\n",
        "\n",
        "# ============================================================================\n",
        "# 8. VISUALIZATIONS\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\nStep 7: Generating visualizations...\")\n",
        "\n",
        "# Plot 1: Sigma correlation heatmap\n",
        "plt.figure(figsize=(8, 6))\n",
        "corr_matrix = np.zeros((K, K))\n",
        "for i in range(K):\n",
        "    for j in range(K):\n",
        "        denom = np.sqrt(Sigma_mean[i, i] * Sigma_mean[j, j])\n",
        "        corr_matrix[i, j] = Sigma_mean[i, j] / denom if denom > 0 else 0\n",
        "\n",
        "sns.heatmap(corr_matrix, annot=True, fmt='.3f', cmap='coolwarm', center=0,\n",
        "            xticklabels=property_names, yticklabels=property_names,\n",
        "            cbar_kws={'label': 'Correlation'}, vmin=-1, vmax=1)\n",
        "plt.title('Posterior Mean Residual Correlation Matrix (Σ)', fontsize=12, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.savefig(f\"{output_dir}/plots/sigma_correlation_heatmap.png\", bbox_inches='tight')\n",
        "print(f\"  - Saved: plots/sigma_correlation_heatmap.png\")\n",
        "plt.close()\n",
        "\n",
        "# Plot 2: Lambda coupling heatmap\n",
        "plt.figure(figsize=(8, 6))\n",
        "corr_lambda = np.zeros((K, K))\n",
        "for i in range(K):\n",
        "    for j in range(K):\n",
        "        denom = np.sqrt(Lambda_mean[i, i] * Lambda_mean[j, j])\n",
        "        corr_lambda[i, j] = Lambda_mean[i, j] / denom if denom > 0 else 0\n",
        "\n",
        "sns.heatmap(corr_lambda, annot=True, fmt='.3f', cmap='viridis',\n",
        "            xticklabels=property_names, yticklabels=property_names,\n",
        "            cbar_kws={'label': 'Coupling'}, vmin=0, vmax=1)\n",
        "plt.title('Posterior Mean Coefficient Coupling Matrix (Λ)', fontsize=12, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.savefig(f\"{output_dir}/plots/lambda_coupling_heatmap.png\", bbox_inches='tight')\n",
        "print(f\"  - Saved: plots/lambda_coupling_heatmap.png\")\n",
        "plt.close()\n",
        "\n",
        "# Plot 3: Global mean coefficients\n",
        "fig, axes = plt.subplots(1, K, figsize=(16, 4), sharey=True)\n",
        "fig.suptitle('Posterior Distributions of Global Mean Coefficients (M)', fontsize=13, fontweight='bold')\n",
        "\n",
        "covariate_names_short = ['Int'] + [c.replace('frac_', '') for c in X_std.columns[1:]]\n",
        "\n",
        "for k in range(K):\n",
        "    means_k = M_mean[:, k]\n",
        "    lower_k = M_lower[:, k]\n",
        "    upper_k = M_upper[:, k]\n",
        "\n",
        "    axes[k].errorbar(means_k, range(p), xerr=[means_k - lower_k, upper_k - means_k],\n",
        "                     fmt='o', markersize=4, capsize=3, capthick=1, alpha=0.8)\n",
        "    axes[k].axvline(0, color='red', linestyle='--', linewidth=0.8, alpha=0.7)\n",
        "    axes[k].set_xlabel('Coefficient Value', fontsize=9)\n",
        "    axes[k].set_title(property_names[k], fontsize=10, fontweight='bold')\n",
        "    axes[k].grid(True, alpha=0.3, axis='x')\n",
        "\n",
        "    if k == 0:\n",
        "        axes[k].set_yticks(range(p))\n",
        "        axes[k].set_yticklabels(covariate_names_short, fontsize=8)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(f\"{output_dir}/plots/global_mean_coefficients.png\", bbox_inches='tight')\n",
        "print(f\"  - Saved: plots/global_mean_coefficients.png\")\n",
        "plt.close()\n",
        "\n",
        "# Plot 4: Class-specific intercepts\n",
        "B_means = {j: np.mean(samples['B'][j], axis=0) for j in range(J)}\n",
        "\n",
        "fig, axes = plt.subplots(1, K, figsize=(16, 4))\n",
        "fig.suptitle('Class-Specific Intercepts by Stability Class', fontsize=13, fontweight='bold')\n",
        "\n",
        "for k in range(K):\n",
        "    intercepts = [B_means[j][0, k] for j in range(J)]\n",
        "    axes[k].bar(range(J), intercepts, color=sns.color_palette(\"Set2\", J), alpha=0.8)\n",
        "    axes[k].set_xticks(range(J))\n",
        "    axes[k].set_xticklabels(stability_classes, rotation=45, ha='right', fontsize=8)\n",
        "    axes[k].set_ylabel('Intercept (log scale)', fontsize=9)\n",
        "    axes[k].set_title(property_names[k], fontsize=10, fontweight='bold')\n",
        "    axes[k].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(f\"{output_dir}/plots/class_specific_intercepts.png\", bbox_inches='tight')\n",
        "print(f\"  - Saved: plots/class_specific_intercepts.png\")\n",
        "plt.close()\n",
        "\n",
        "# Plot 5: Predictive distributions\n",
        "fig, axes = plt.subplots(n_test, K, figsize=(16, max(10, n_test*2)))\n",
        "fig.suptitle('Posterior Predictive Distributions (Original Scale)', fontsize=14, fontweight='bold')\n",
        "\n",
        "for t_idx in range(n_test):\n",
        "    for k in range(K):\n",
        "        samples_tk = predictive_samples_original[:, t_idx, k]\n",
        "\n",
        "        obs_val = df_clean.iloc[test_indices[t_idx]][property_names[k]]\n",
        "\n",
        "        axes[t_idx, k].hist(samples_tk, bins=30, alpha=0.7, color='steelblue', edgecolor='black')\n",
        "\n",
        "        if not np.isnan(obs_val):\n",
        "            axes[t_idx, k].axvline(obs_val, color='red', linestyle='--', linewidth=2,\n",
        "                                   label=f'Observed: {obs_val:.1f}')\n",
        "            axes[t_idx, k].legend(fontsize=7, loc='upper right')\n",
        "\n",
        "        pred_mean = np.mean(samples_tk)\n",
        "        axes[t_idx, k].axvline(pred_mean, color='green', linestyle='-', linewidth=1.5,\n",
        "                               label=f'Pred Mean: {pred_mean:.1f}')\n",
        "\n",
        "        if k == 0:\n",
        "            alloy_name = str(df_clean.iloc[test_indices[t_idx]]['eid'])\n",
        "            axes[t_idx, k].set_ylabel(f'{alloy_name[:15]}...', fontsize=8)\n",
        "\n",
        "        if t_idx == 0:\n",
        "            axes[t_idx, k].set_title(property_names[k], fontsize=10, fontweight='bold')\n",
        "\n",
        "        axes[t_idx, k].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(f\"{output_dir}/plots/predictive_distributions.png\", bbox_inches='tight')\n",
        "print(f\"  - Saved: plots/predictive_distributions.png\")\n",
        "plt.close()\n",
        "\n",
        "# Plot 6: Property trade-offs\n",
        "Z_mean_post = np.mean(samples['Z'], axis=0)\n",
        "Z_original_mean = np.exp(Z_mean_post)\n",
        "\n",
        "pairs = [(0, 1), (1, 3), (2, 4)]\n",
        "pair_names = [\n",
        "    (property_names[pairs[0][0]], property_names[pairs[0][1]]),\n",
        "    (property_names[pairs[1][0]], property_names[pairs[1][1]]),\n",
        "    (property_names[pairs[2][0]], property_names[pairs[2][1]]),\n",
        "]\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
        "fig.suptitle('Property Trade-offs (Posterior Mean, Original Scale)', fontsize=13, fontweight='bold')\n",
        "\n",
        "for i, (p1, p2) in enumerate(pairs):\n",
        "    colors = [sns.color_palette(\"Set2\", J)[z_class[j]] for j in range(n)]\n",
        "\n",
        "    axes[i].scatter(Z_original_mean[:, p1], Z_original_mean[:, p2],\n",
        "                    c=colors, alpha=0.6, s=30, edgecolors='k', linewidths=0.3)\n",
        "    axes[i].set_xlabel(pair_names[i][0], fontsize=10)\n",
        "    axes[i].set_ylabel(pair_names[i][1], fontsize=10)\n",
        "    axes[i].grid(True, alpha=0.3)\n",
        "\n",
        "from matplotlib.patches import Patch\n",
        "legend_elements = [Patch(facecolor=sns.color_palette(\"Set2\", J)[j], label=stability_classes[j])\n",
        "                   for j in range(J)]\n",
        "fig.legend(handles=legend_elements, loc='upper center', bbox_to_anchor=(0.5, -0.05),\n",
        "           ncol=J, fontsize=9, title='Stability Class')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(f\"{output_dir}/plots/property_tradeoffs.png\", bbox_inches='tight')\n",
        "print(f\"  - Saved: plots/property_tradeoffs.png\")\n",
        "plt.close()\n",
        "\n",
        "# Plot 7: Observed vs predicted\n",
        "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
        "fig.suptitle('Observed vs Predicted with 95% Credible Intervals', fontsize=13, fontweight='bold')\n",
        "axes = axes.flatten()\n",
        "\n",
        "for k in range(K):\n",
        "    pred_mean_all = np.mean(samples['Z'], axis=0)[:, k]\n",
        "    pred_lower_all = np.percentile(samples['Z'], 2.5, axis=0)[:, k]\n",
        "    pred_upper_all = np.percentile(samples['Z'], 97.5, axis=0)[:, k]\n",
        "\n",
        "    pred_mean_orig = np.exp(pred_mean_all)\n",
        "    pred_lower_orig = np.exp(pred_lower_all)\n",
        "    pred_upper_orig = np.exp(pred_upper_all)\n",
        "\n",
        "    obs_orig = df_clean[property_names[k]].values\n",
        "\n",
        "    valid = ~np.isnan(obs_orig) & (obs_orig > 0)\n",
        "\n",
        "    if valid.sum() > 0:\n",
        "        axes[k].errorbar(obs_orig[valid], pred_mean_orig[valid],\n",
        "                         yerr=[pred_mean_orig[valid] - pred_lower_orig[valid],\n",
        "                               pred_upper_orig[valid] - pred_mean_orig[valid]],\n",
        "                         fmt='o', markersize=3, alpha=0.5, capsize=2, elinewidth=0.5)\n",
        "\n",
        "        lims = [min(obs_orig[valid].min(), pred_mean_orig[valid].min()),\n",
        "                max(obs_orig[valid].max(), pred_mean_orig[valid].max())]\n",
        "        axes[k].plot(lims, lims, 'r--', alpha=0.8, linewidth=1.5)\n",
        "\n",
        "    axes[k].set_xlabel(f'Observed {property_names[k]}', fontsize=9)\n",
        "    axes[k].set_ylabel(f'Predicted {property_names[k]}', fontsize=9)\n",
        "    axes[k].set_title(property_names[k], fontsize=10, fontweight='bold')\n",
        "    axes[k].grid(True, alpha=0.3)\n",
        "\n",
        "axes[-1].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(f\"{output_dir}/plots/observed_vs_predicted.png\", bbox_inches='tight')\n",
        "print(f\"  - Saved: plots/observed_vs_predicted.png\")\n",
        "plt.close()\n",
        "\n",
        "# ============================================================================\n",
        "# 9. TABLES\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\nStep 8: Generating summary tables...\")\n",
        "\n",
        "# Table 1: Posterior summaries\n",
        "table1_data = {\n",
        "    'Parameter': [\n",
        "        r'tau^2',\n",
        "        r'Sigma[YM,YM]',\n",
        "        r'Sigma[YS,YS]',\n",
        "        r'Sigma[UTS,UTS]',\n",
        "        r'Lambda[YM,YM]',\n",
        "        r'M[intercept,YM]',\n",
        "        r'M[intercept,YS]',\n",
        "    ],\n",
        "    'Mean': [\n",
        "        tau2_mean,\n",
        "        Sigma_mean[0, 0],\n",
        "        Sigma_mean[1, 1],\n",
        "        Sigma_mean[2, 2],\n",
        "        Lambda_mean[0, 0],\n",
        "        M_mean[0, 0],\n",
        "        M_mean[0, 1],\n",
        "    ],\n",
        "    'Std': [\n",
        "        tau2_std,\n",
        "        Sigma_std[0, 0],\n",
        "        Sigma_std[1, 1],\n",
        "        Sigma_std[2, 2],\n",
        "        np.std(samples['Lambda'][:, 0, 0]),\n",
        "        M_std[0, 0],\n",
        "        M_std[0, 1],\n",
        "    ],\n",
        "    '2.5%': [\n",
        "        np.percentile(samples['tau2'], 2.5),\n",
        "        np.percentile(samples['Sigma'][:, 0, 0], 2.5),\n",
        "        np.percentile(samples['Sigma'][:, 1, 1], 2.5),\n",
        "        np.percentile(samples['Sigma'][:, 2, 2], 2.5),\n",
        "        np.percentile(samples['Lambda'][:, 0, 0], 2.5),\n",
        "        M_lower[0, 0],\n",
        "        M_lower[0, 1],\n",
        "    ],\n",
        "    '97.5%': [\n",
        "        np.percentile(samples['tau2'], 97.5),\n",
        "        np.percentile(samples['Sigma'][:, 0, 0], 97.5),\n",
        "        np.percentile(samples['Sigma'][:, 1, 1], 97.5),\n",
        "        np.percentile(samples['Sigma'][:, 2, 2], 97.5),\n",
        "        np.percentile(samples['Lambda'][:, 0, 0], 97.5),\n",
        "        M_upper[0, 0],\n",
        "        M_upper[0, 1],\n",
        "    ],\n",
        "}\n",
        "\n",
        "df_table1 = pd.DataFrame(table1_data)\n",
        "df_table1.to_csv(f\"{output_dir}/tables/posterior_parameter_summary.csv\", index=False)\n",
        "print(f\"  - Saved: tables/posterior_parameter_summary.csv\")\n",
        "print(\"\\n\" + df_table1.to_string(index=False))\n",
        "\n",
        "# Table 2: Predictive performance\n",
        "rmse_values = []\n",
        "coverage_values = []\n",
        "\n",
        "for k in range(K):\n",
        "    obs_k = df_clean[property_names[k]].values\n",
        "    valid_k = ~np.isnan(obs_k) & (obs_k > 0)\n",
        "\n",
        "    if valid_k.sum() == 0:\n",
        "        rmse_values.append(np.nan)\n",
        "        coverage_values.append(np.nan)\n",
        "        continue\n",
        "\n",
        "    pred_mean_k = np.exp(np.mean(samples['Z'], axis=0)[:, k])\n",
        "    pred_lower_k = np.exp(np.percentile(samples['Z'], 2.5, axis=0)[:, k])\n",
        "    pred_upper_k = np.exp(np.percentile(samples['Z'], 97.5, axis=0)[:, k])\n",
        "\n",
        "    rmse = np.sqrt(np.mean((obs_k[valid_k] - pred_mean_k[valid_k])**2))\n",
        "    rmse_values.append(rmse)\n",
        "\n",
        "    coverage = np.mean((obs_k[valid_k] >= pred_lower_k[valid_k]) &\n",
        "                       (obs_k[valid_k] <= pred_upper_k[valid_k]))\n",
        "    coverage_values.append(coverage)\n",
        "\n",
        "table2_data = {\n",
        "    'Property': property_names,\n",
        "    'RMSE': rmse_values,\n",
        "    '95% CI Coverage': coverage_values,\n",
        "}\n",
        "\n",
        "df_table2 = pd.DataFrame(table2_data)\n",
        "df_table2.to_csv(f\"{output_dir}/tables/predictive_performance.csv\", index=False)\n",
        "print(f\"  - Saved: tables/predictive_performance.csv\")\n",
        "print(\"\\n\" + df_table2.to_string(index=False))\n",
        "\n",
        "# Table 3: Class-specific intercepts\n",
        "intercept_summary = []\n",
        "for j in range(J):\n",
        "    B_j_samples = samples['B'][j]\n",
        "    for k in range(K):\n",
        "        intercept_samples = B_j_samples[:, 0, k]\n",
        "        intercept_summary.append({\n",
        "            'Stability Class': stability_classes[j],\n",
        "            'Property': property_names[k],\n",
        "            'Mean': np.mean(intercept_samples),\n",
        "            'Std': np.std(intercept_samples),\n",
        "            '2.5%': np.percentile(intercept_samples, 2.5),\n",
        "            '97.5%': np.percentile(intercept_samples, 97.5),\n",
        "        })\n",
        "\n",
        "df_table3 = pd.DataFrame(intercept_summary)\n",
        "df_table3.to_csv(f\"{output_dir}/tables/class_specific_intercepts.csv\", index=False)\n",
        "print(f\"  - Saved: tables/class_specific_intercepts.csv\")\n",
        "print(\"\\n\" + df_table3.head(10).to_string(index=False))\n",
        "\n",
        "# ============================================================================\n",
        "# 10. CREATE ZIP ARCHIVE\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\nStep 9: Creating ZIP archive of all outputs...\")\n",
        "\n",
        "zip_filename = f\"{output_dir}.zip\"\n",
        "with zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "    for root, dirs, files in os.walk(output_dir):\n",
        "        for file in files:\n",
        "            file_path = os.path.join(root, file)\n",
        "            arcname = os.path.relpath(file_path, output_dir)\n",
        "            zipf.write(file_path, arcname)\n",
        "\n",
        "print(f\"  - Created: {zip_filename}\")\n",
        "\n",
        "# ============================================================================\n",
        "# 11. FINAL SUMMARY\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"MCMC ANALYSIS COMPLETED SUCCESSFULLY!\")\n",
        "print(\"=\"*80)\n",
        "print(f\"\\nAll outputs saved to: {output_dir}/\")\n",
        "print(f\"Compressed archive: {zip_filename}\")\n",
        "print(f\"\\nSamples collected: {n_save}\")\n",
        "print(f\"Numerical errors handled: {n_errors}\")\n",
        "print(\"\\nGenerated outputs:\")\n",
        "print(\"  Plots (7)\")\n",
        "print(\"  Diagnostics (3)\")\n",
        "print(\"  Tables (3)\")\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"END OF ANALYSIS\")\n",
        "print(\"=\"*80)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nVyg9rjq01L0",
        "outputId": "c7a47bd9-15d3-4371-d01b-5393d8a9bad9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "PHYSICS-INFORMED BAYESIAN MCMC FOR TI-ALLOY PROPERTY PREDICTION\n",
            "================================================================================\n",
            "\n",
            "Output directory: ti_alloy_bayesian_outputs_20251119_015559\n",
            "\n",
            "Step 1: Loading and preprocessing DAX-Ti database...\n",
            "  - Loaded 283 alloy entries\n",
            "  - Prepared 283 samples with 9 covariates\n",
            "  - Covariates: ['intercept', 'moe', 'frac_Nb', 'frac_Zr', 'frac_Fe', 'frac_Mo', 'frac_Ta', 'frac_Sn', 'frac_V']\n",
            "  - Stability classes: ['stable', 'meta', 'near', 'rich', 'other']\n",
            "  - Class distribution: {'stable': 15, 'meta': 150, 'near': 77, 'rich': 31, 'other': 10}\n",
            "\n",
            "  Missing data per property:\n",
            "    YM: 16.3%\n",
            "    YS: 4.2%\n",
            "    UTS: 30.4%\n",
            "    DAR: 32.2%\n",
            "    HV: 46.3%\n",
            "\n",
            "Step 2: Initializing MCMC parameters...\n",
            "  - Initialized all parameters with regularization\n",
            "\n",
            "  Hyperparameters:\n",
            "    c0 (M prior scale): 1.0\n",
            "    nu0 (Sigma prior df): 10\n",
            "    nu_Lambda (Lambda prior df): 10\n",
            "    a_tau, b_tau: 3.0, 1.0\n",
            "\n",
            "Step 3: Running MCMC sampler...\n",
            "  This may take several minutes...\n",
            "\n",
            "  Progress: [==================================================] 100% (5000/5000)\n",
            "  MCMC sampling completed!\n",
            "  Total numerical errors handled: 0\n",
            "\n",
            "Step 4: Computing posterior summaries...\n",
            "  - Posterior mean tau^2: 10.0000 ± 0.0000\n",
            "  - Posterior mean Sigma diagonal: [0.0009352  0.00079475 0.00087375 0.00083932 0.00065675]\n",
            "\n",
            "Step 5: Computing posterior predictive distributions...\n",
            "  - Generated 2000 predictive samples for 5 test cases\n",
            "\n",
            "Step 6: Computing MCMC diagnostics...\n",
            "  - Saved: diagnostics/trace_plots.png\n",
            "  - Saved: diagnostics/autocorrelation_plots.png\n",
            "\n",
            "  Effective Sample Sizes:\n",
            "    tau^2: 2\n",
            "    Sigma[0,0]: 2000\n",
            "    M[0,0]: 29\n",
            "  - Saved: diagnostics/rhat_diagnostics.png\n",
            "\n",
            "Step 7: Generating visualizations...\n",
            "  - Saved: plots/sigma_correlation_heatmap.png\n",
            "  - Saved: plots/lambda_coupling_heatmap.png\n",
            "  - Saved: plots/global_mean_coefficients.png\n",
            "  - Saved: plots/class_specific_intercepts.png\n",
            "  - Saved: plots/predictive_distributions.png\n",
            "  - Saved: plots/property_tradeoffs.png\n",
            "  - Saved: plots/observed_vs_predicted.png\n",
            "\n",
            "Step 8: Generating summary tables...\n",
            "  - Saved: tables/posterior_parameter_summary.csv\n",
            "\n",
            "      Parameter      Mean      Std      2.5%     97.5%\n",
            "          tau^2 10.000000 0.000000 10.000000 10.000000\n",
            "   Sigma[YM,YM]  0.000935 0.000071  0.000807  0.001084\n",
            "   Sigma[YS,YS]  0.000795 0.000064  0.000677  0.000928\n",
            " Sigma[UTS,UTS]  0.000874 0.000068  0.000749  0.001015\n",
            "  Lambda[YM,YM]  0.001556 0.000586  0.000668  0.002918\n",
            "M[intercept,YM]  4.529116 0.051078  4.426058  4.631521\n",
            "M[intercept,YS]  6.677707 0.055327  6.563586  6.784856\n",
            "  - Saved: tables/predictive_performance.csv\n",
            "\n",
            "Property       RMSE  95% CI Coverage\n",
            "      YM  13.724163         0.362869\n",
            "      YS 234.005981         0.173432\n",
            "     UTS 274.955793         0.177665\n",
            "     DAR  12.117818         0.062500\n",
            "      HV  48.537786         0.289474\n",
            "  - Saved: tables/class_specific_intercepts.csv\n",
            "\n",
            "Stability Class Property     Mean      Std     2.5%    97.5%\n",
            "         stable       YM 5.859056 0.186435 5.513821 6.303246\n",
            "         stable       YS 6.696307 0.043565 6.613336 6.784593\n",
            "         stable      UTS 6.762218 0.051886 6.664455 6.862567\n",
            "         stable      DAR 0.706096 0.244377 0.298961 1.164137\n",
            "         stable       HV 5.609718 0.057647 5.493380 5.723865\n",
            "           meta       YM 4.358098 0.006003 4.346763 4.369817\n",
            "           meta       YS 6.451806 0.005872 6.440785 6.463797\n",
            "           meta      UTS 6.613117 0.007123 6.599054 6.626873\n",
            "           meta      DAR 3.043910 0.008520 3.027375 3.061403\n",
            "           meta       HV 5.695930 0.005038 5.686335 5.705895\n",
            "\n",
            "Step 9: Creating ZIP archive of all outputs...\n",
            "  - Created: ti_alloy_bayesian_outputs_20251119_015559.zip\n",
            "\n",
            "================================================================================\n",
            "MCMC ANALYSIS COMPLETED SUCCESSFULLY!\n",
            "================================================================================\n",
            "\n",
            "All outputs saved to: ti_alloy_bayesian_outputs_20251119_015559/\n",
            "Compressed archive: ti_alloy_bayesian_outputs_20251119_015559.zip\n",
            "\n",
            "Samples collected: 2000\n",
            "Numerical errors handled: 0\n",
            "\n",
            "Generated outputs:\n",
            "  Plots (7)\n",
            "  Diagnostics (3)\n",
            "  Tables (3)\n",
            "\n",
            "================================================================================\n",
            "END OF ANALYSIS\n",
            "================================================================================\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPp946B/CrtW41Z9Dq41JrK",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}